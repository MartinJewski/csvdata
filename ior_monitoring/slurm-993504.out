---------------------------------
---------------------------------
Node_LIST: node25-008 node26-007 node27-018 node30-002 node30-003 node30-006 node34-014
-------------------iter:0------------------------
monitoring start 0 PID: 44151
--------------------------------------------------
-------------------iter:1------------------------
distribute from: node26-007
monitoring start 1 PID: 44153
--------------------------------------------------
-------------------iter:2------------------------
distribute from: node27-018
monitoring start 2 PID: 44155
--------------------------------------------------
-------------------iter:3------------------------
distribute from: node30-002
monitoring start 3 PID: 44157
--------------------------------------------------
-------------------iter:4------------------------
distribute from: node30-003
monitoring start 4 PID: 44158
--------------------------------------------------
-------------------iter:5------------------------
distribute from: node30-006
monitoring start 5 PID: 44161
--------------------------------------------------
-------------------iter:6------------------------
distribute from: node34-014
monitoring start 6 PID: 44162
--------------------------------------------------
distribute from: node25-008
main tool program PID: 44284
MY MAIN_PID: 44284
MY PID_LIST: 44151 44153 44155 44157 44158 44161 44162
IOR-3.3.0: MPI Coordinated Test of Parallel I/O
Began               : Sun Apr  9 02:37:10 2023
Command line        : ior -a MPIIO -t 1000M -b 1000M -C -F -e
Machine             : Linux node25-008.cm.cluster
TestID              : 0
StartTime           : Sun Apr  9 02:37:10 2023
Path                : /home/fuchs/aglippert/machajewskim/development/node-level-resource-monitoring/src
FS                  : 36.2 TiB   Used FS: 10.7%   Inodes: 581.9 Mi   Used Inodes: 7.3%

Options: 
api                 : MPIIO
apiVersion          : (3.1)
test filename       : testFile
access              : file-per-process
type                : independent
segments            : 1
ordering in a file  : sequential
ordering inter file : constant task offset
task offset         : 1
nodes               : 7
tasks               : 120
clients per node    : 18
repetitions         : 1
xfersize            : 1000 MiB
blocksize           : 1000 MiB
aggregate filesize  : 117.19 GiB

Results: 

access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----
WARNING: Expected aggregate file size       = 125829120000.
WARNING: Stat() of aggregate file size      = 27218911232.
WARNING: Using actual aggregate bytes moved = 125829120000.
write     746.37     0.746427   153.52      1024000    1024000    0.028502   160.77     20.74      160.78     0   
WARNING: Expected aggregate file size       = 125829120000.
WARNING: Stat() of aggregate file size      = 27218911232.
WARNING: Using actual aggregate bytes moved = 125829120000.
read      3412.17    3.42       3.77        1024000    1024000    31.39      35.06      32.91      35.17      0   
remove    -          -          -           -          -          -          -          -          4.83       0   
Max Write: 746.37 MiB/sec (782.63 MB/sec)
Max Read:  3412.17 MiB/sec (3577.92 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev   Max(OPs)   Min(OPs)  Mean(OPs)     StdDev    Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt   blksiz    xsize aggs(MiB)   API RefNum
write         746.37     746.37     746.37       0.00       0.75       0.75       0.75       0.00  160.77750         NA            NA     0    120  18    1   1     1        1         0    0      1 1048576000 1048576000  120000.0 MPIIO      0
read         3412.17    3412.17    3412.17       0.00       3.41       3.41       3.41       0.00   35.16826         NA            NA     0    120  18    1   1     1        1         0    0      1 1048576000 1048576000  120000.0 MPIIO      0
Finished            : Sun Apr  9 02:40:32 2023
TERMINATE METRIC JOB - PID: 44151
./jobdistributor.sh: line 123: kill: (44151) - No such process
TERMINATE METRIC JOB - PID: 44153
./jobdistributor.sh: line 132: 44153 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
TERMINATE METRIC JOB - PID: 44155
./jobdistributor.sh: line 132: 44155 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
TERMINATE METRIC JOB - PID: 44157
./jobdistributor.sh: line 132: 44157 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
TERMINATE METRIC JOB - PID: 44158
./jobdistributor.sh: line 132: 44158 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
TERMINATE METRIC JOB - PID: 44161
./jobdistributor.sh: line 132: 44161 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
TERMINATE METRIC JOB - PID: 44162
./jobdistributor.sh: line 132: 44162 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
\n
