---------------------------------
---------------------------------
Node_LIST: node28-001 node28-002 node28-003 node28-004
-------------------iter:0------------------------
monitoring start 0 PID: 60367
--------------------------------------------------
-------------------iter:1------------------------
distribute from: node28-002
monitoring start 1 PID: 60369
--------------------------------------------------
-------------------iter:2------------------------
distribute from: node28-003
monitoring start 2 PID: 60371
--------------------------------------------------
-------------------iter:3------------------------
distribute from: node28-004
monitoring start 3 PID: 60373
--------------------------------------------------
distribute from: node28-001
main tool program PID: 60464
MY MAIN_PID: 60464
MY PID_LIST: 60367 60369 60371 60373
IOR-3.3.0: MPI Coordinated Test of Parallel I/O
Began               : Fri Apr 14 15:36:50 2023
Command line        : ior -a MPIIO -t 500M -b 1000M -F -o /scratch/fuchs/aglippert/machajewskim/testoutput
Machine             : Linux node28-001.cm.cluster
TestID              : 0
StartTime           : Fri Apr 14 15:36:50 2023
Path                : /scratch/fuchs/aglippert/machajewskim
FS                  : 2415.8 TiB   Used FS: 19.6%   Inodes: 0.0 Mi   Used Inodes: -nan%

Options: 
api                 : MPIIO
apiVersion          : (3.1)
test filename       : /scratch/fuchs/aglippert/machajewskim/testoutput
access              : file-per-process
type                : independent
segments            : 1
ordering in a file  : sequential
ordering inter file : no tasks offsets
nodes               : 4
tasks               : 80
clients per node    : 20
repetitions         : 1
xfersize            : 500 MiB
blocksize           : 1000 MiB
aggregate filesize  : 78.12 GiB

Results: 

access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----
write     6140       12.29      4.40        1024000    512000     0.057198   13.02      4.21       13.03      0   
read      6140       12.29      4.91        1024000    512000     0.099346   13.02      3.54       13.03      0   
remove    -          -          -           -          -          -          -          -          0.028328   0   
Max Write: 6139.80 MiB/sec (6438.05 MB/sec)
Max Read:  6139.94 MiB/sec (6438.19 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev   Max(OPs)   Min(OPs)  Mean(OPs)     StdDev    Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt   blksiz    xsize aggs(MiB)   API RefNum
write        6139.80    6139.80    6139.80       0.00      12.28      12.28      12.28       0.00   13.02974         NA            NA     0     80  20    1   1     0        1         0    0      1 1048576000 524288000   80000.0 MPIIO      0
read         6139.94    6139.94    6139.94       0.00      12.28      12.28      12.28       0.00   13.02945         NA            NA     0     80  20    1   1     0        1         0    0      1 1048576000 524288000   80000.0 MPIIO      0
Finished            : Fri Apr 14 15:37:16 2023
TERMINATE METRIC JOB - PID: 60367
./jobdistributor.sh: line 123: kill: (60367) - No such process
TERMINATE METRIC JOB - PID: 60369
./jobdistributor.sh: line 132: 60369 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
TERMINATE METRIC JOB - PID: 60371
./jobdistributor.sh: line 132: 60371 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
TERMINATE METRIC JOB - PID: 60373
./jobdistributor.sh: line 132: 60373 Killed                  mpirun --oversubscribe -npernode 1 -H ${NODE_LIST[$i]} python3 $(pwd)/gstart.py "$2" >> workermon${NODE_LIST[$i]}.txt
\n
